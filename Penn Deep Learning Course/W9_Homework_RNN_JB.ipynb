{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"W9_Homework_RNN_JB.ipynb","provenance":[{"file_id":"1g60cEC4qJyobucjx52ZMwKc1Xue0OGKH","timestamp":1617029223156},{"file_id":"https://github.com/CIS-522/course-content/blob/main/tutorials/W9_RNN/W9_Homework.ipynb","timestamp":1617029192261}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"KKgzsidLGINz"},"source":["# Week 9 Homework"]},{"cell_type":"code","metadata":{"id":"d1GTaygz4aqK","cellView":"form","executionInfo":{"status":"ok","timestamp":1620138122226,"user_tz":240,"elapsed":396,"user":{"displayName":"Justin Brantley","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggf20F3pZnsjHk2N7sW7COs-iVwGzxmFgeR7224=s64","userId":"14927837600437153536"}}},"source":["#@markdown What is your Pennkey and pod? (text, not numbers, e.g. bfranklin)\n","my_pennkey = 'jabrantl' #@param {type:\"string\"}\n","my_pod = 'superfluous-lyrebird' #@param ['Select', 'euclidean-wombat', 'sublime-newt', 'buoyant-unicorn', 'lackadaisical-manatee','indelible-stingray','superfluous-lyrebird','discreet-reindeer','quizzical-goldfish','astute-jellyfish','ubiquitous-cheetah','nonchalant-crocodile','fashionable-lemur','spiffy-eagle','electric-emu','quotidian-lion']\n"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ajtPqks9C1Qp"},"source":["## Section 1: Role of social media platforms in mitigating hate speech"]},{"cell_type":"markdown","metadata":{"id":"zzCVG642DFed"},"source":["Social media platforms have instituted not just user policies for online conduct, but also technological mechanisms to identify and remove hate speech.  As an example, review Facebook’s discussion of how its employing AI to identify hate speech.  As you read this article, pay particular attention to the specific challenges they enumerate in identifying hate speech. \n","\n","\"AI advances to better detect hate speech\" \n","\n","https://ai.facebook.com/blog/ai-advances-to-better-detect-hate-speech/\n","\n","*Facebook, 2020* [10-minute read]\n","\n","The Facebook article emphasizes that the challenges to identifying hate speech are both social and technological.  The social considerations include the importance of cultural context as well as the nuances of languages. How does that resonate with the \"Excavating AI: The Politics of Images in Machine Learning Training Sets\" article you read in Week 6?  What do you think of Facebook’s approach? (200-300 words)"]},{"cell_type":"code","metadata":{"id":"MOLhvJ8E4pYD","cellView":"form"},"source":["section1_ans = \"This problem seems like an almost insurmountable challenge. To me, ~90% accuracy detection of hate speech is quite impressive considering all of the nuance and slang that is often used in hateful speech. Examples of direct threats or hate speech seems somewhat simple to detect, but in many cases, hate speech/writing can occur through difficult to recognize bullying. For example, the idea of combining ambiguous text onto images to create hateful memes seems like an incredibly difficult challenge. The one concern in all of this is how the speech is flagged as 'hateful' in the first place. In fact, many people (particularly those in more conservative circles) often complain about their speech being flagged as hateful and companies overstepping and violating the 1st amendment. However,  hate speech is not protected by the 1st amendment  , so it is right to be removed. On the other hand, the definition of hate speech can be manipulated to reflect the bias of those doing the labeling. I like the idea of self-supervised learning as a way to identify potential hate speech w/o humans in the loop, but this could also lead to accidental suppression of speech that was not actually hate speech or was quoting or rebuttal to hate speech.   The second part of the article is more challenging for me. I see how the discussion about language styles being perceived as 'wrong' in a social media environment is bad, but I still struggle with the idea of having vernacular that uses complete improper grammar as being correct English. I also sometimes struggle with the criticism that you have to \\\"talk White\\\" to be accepted as talking correctly. My family speaks English and Spanish and in both languages, grammar and structure are still present and somewhat interchangeable between both. I find the argument about double negatives to be weak unless it purely hinges on the word 'logic'. Maybe illogical is incorrect, but just because something is ok in another language does not mean that it has to hold for a different one. Just because you can do it in Italian does not mean that it is also ok in English. That would also mean that in  \" #@param {type:\"string\"}"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fIgRA_GXRkLh"},"source":["## Section 2: Broader societal considerations of addressing hate speech on social media\n"]},{"cell_type":"markdown","metadata":{"id":"0_J--GOXC4Bj"},"source":["Next, read this Medium article about the unintended consequences of technological interventions to address hate speech on social media.\n","\n","\"How Automated Tools Discriminate Against Black Language\"    \n","https://medium.com/@annawchung/how-automated-tools-discriminate-against-black-language-2ac8eab8d6db.  \n","*Medium, 2019* [14-minute read]\n","\n","\"Challenges in Automated Debiasing for Toxic Language Detection\"    \n","https://arxiv.org/pdf/2102.00086.pdf.  \n","*Optional – this is much longer and more technical!!!*\n","\n","How does this example illustrate the complexity of identifying hate speech?  What solutions does the author suggest? Why is it important that AI practitioners have a robust understanding of the societal context and consequence of their work? (1-2 paragraphs)"]},{"cell_type":"code","metadata":{"cellView":"form","id":"0HhP9MqW41tY"},"source":["section2_ans = 'Clearly the method for labeling text as toxic is completely flawed and has huge racial, gender, and sexual identity bias. Although I do not know the technical side of how these expressions are being labeled as toxic, it is not clear to me at all how this could occur in the first place w/o having seriously corrupted training data--i.e., people were labeling things with their own biases being very strong. I find the second part of the article to be less compelling, but perhaps that discussion is for a later time.   ' #@param {type:\"string\"}"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"d3A3u4yjC6B-"},"source":["## Section 3: Ablation Study on LSTMs"]},{"cell_type":"markdown","metadata":{"id":"HujG0E6VUXzx"},"source":["In the tutorials we learnt about the different gates in LSTMs. We also learnt about GRUs that have fewer components than LSTMs but work reasonably well. In this section of the homework you will conduct an ablation study for LSTMs. You will be working on text generation task. The model will be trained on H. G. Wells’ *The Time Machine*.\n","\n","In order to have a baseline, first intialize the weights and implement the gates of a LSTM. \n","\n","For the ablation study change different components listed below (these are suggestions, you are free to use your own ideas as well) and observe the effects on performance. \n","- Initialization\n","- Number of gates\n","- Replacing sigmoid, tanh with other functions\n","- Changing the inputs used for each recurrent unit\n","\n","You will make changes in the `get_lstm_params` and `lstm` functions. We have provided code for forward and backward propagation, training and a sample code to see the text generation results. \n","(Please remember to to return tensors with the same shape as the LSTM you will implement to ensure end to end training.)"]},{"cell_type":"code","metadata":{"id":"C8F7j01s3eu_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1620138129952,"user_tz":240,"elapsed":8111,"user":{"displayName":"Justin Brantley","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggf20F3pZnsjHk2N7sW7COs-iVwGzxmFgeR7224=s64","userId":"14927837600437153536"}},"outputId":"990a6d6f-5a96-4c91-fc0c-a4af8805e49b"},"source":["#@title Imports\n","!pip install d2l\n","\n","import torch\n","from torch import nn\n","from d2l import torch as d2l\n","from torch.nn import functional as F\n","import math"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Collecting d2l\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/69/62/a53c44bf0d076df002e4ab16f0cfaf5c31c1b158d0ec88db1494dfbfa5af/d2l-0.16.3-py3-none-any.whl (77kB)\n","\r\u001b[K     |████▎                           | 10kB 13.8MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 20kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 30kB 5.2MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 40kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 51kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 61kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 71kB 3.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 81kB 2.6MB/s \n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from d2l) (1.1.5)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from d2l) (3.2.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from d2l) (1.19.5)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from d2l) (2.23.0)\n","Requirement already satisfied: jupyter in /usr/local/lib/python3.7/dist-packages (from d2l) (1.0.0)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->d2l) (2018.9)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->d2l) (2.8.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->d2l) (1.3.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->d2l) (0.10.0)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->d2l) (2.4.7)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->d2l) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->d2l) (2020.12.5)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->d2l) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->d2l) (3.0.4)\n","Requirement already satisfied: qtconsole in /usr/local/lib/python3.7/dist-packages (from jupyter->d2l) (5.0.3)\n","Requirement already satisfied: notebook in /usr/local/lib/python3.7/dist-packages (from jupyter->d2l) (5.3.1)\n","Requirement already satisfied: ipykernel in /usr/local/lib/python3.7/dist-packages (from jupyter->d2l) (4.10.1)\n","Requirement already satisfied: ipywidgets in /usr/local/lib/python3.7/dist-packages (from jupyter->d2l) (7.6.3)\n","Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from jupyter->d2l) (5.6.1)\n","Requirement already satisfied: jupyter-console in /usr/local/lib/python3.7/dist-packages (from jupyter->d2l) (5.2.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->d2l) (1.15.0)\n","Requirement already satisfied: jupyter-core in /usr/local/lib/python3.7/dist-packages (from qtconsole->jupyter->d2l) (4.7.1)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from qtconsole->jupyter->d2l) (2.6.1)\n","Requirement already satisfied: traitlets in /usr/local/lib/python3.7/dist-packages (from qtconsole->jupyter->d2l) (5.0.5)\n","Requirement already satisfied: pyzmq>=17.1 in /usr/local/lib/python3.7/dist-packages (from qtconsole->jupyter->d2l) (22.0.3)\n","Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from qtconsole->jupyter->d2l) (0.2.0)\n","Requirement already satisfied: jupyter-client>=4.1 in /usr/local/lib/python3.7/dist-packages (from qtconsole->jupyter->d2l) (5.3.5)\n","Requirement already satisfied: qtpy in /usr/local/lib/python3.7/dist-packages (from qtconsole->jupyter->d2l) (1.9.0)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter->d2l) (2.11.3)\n","Requirement already satisfied: nbformat in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter->d2l) (5.1.3)\n","Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter->d2l) (1.5.0)\n","Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter->d2l) (0.9.4)\n","Requirement already satisfied: tornado>=4 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter->d2l) (5.1.1)\n","Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel->jupyter->d2l) (5.5.0)\n","Requirement already satisfied: jupyterlab-widgets>=1.0.0; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jupyter->d2l) (1.0.0)\n","Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jupyter->d2l) (3.5.1)\n","Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->d2l) (3.3.0)\n","Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->d2l) (0.3)\n","Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->d2l) (0.8.4)\n","Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->d2l) (1.4.3)\n","Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->d2l) (0.4.4)\n","Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->d2l) (0.7.1)\n","Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from jupyter-console->jupyter->d2l) (1.0.18)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->notebook->jupyter->d2l) (1.1.1)\n","Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.7/dist-packages (from nbformat->notebook->jupyter->d2l) (2.6.0)\n","Requirement already satisfied: ptyprocess; os_name != \"nt\" in /usr/local/lib/python3.7/dist-packages (from terminado>=0.8.1->notebook->jupyter->d2l) (0.7.0)\n","Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->d2l) (56.0.0)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->d2l) (0.7.5)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->d2l) (4.4.2)\n","Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->d2l) (0.8.1)\n","Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->d2l) (4.8.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->jupyter->d2l) (20.9)\n","Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->jupyter->d2l) (0.5.1)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.0->jupyter-console->jupyter->d2l) (0.2.5)\n","Installing collected packages: d2l\n","Successfully installed d2l-0.16.3\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"MiJvAbGP3pup","executionInfo":{"status":"ok","timestamp":1620138136740,"user_tz":240,"elapsed":267,"user":{"displayName":"Justin Brantley","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggf20F3pZnsjHk2N7sW7COs-iVwGzxmFgeR7224=s64","userId":"14927837600437153536"}}},"source":["def get_lstm_params(vocab_size, num_hiddens):\n","    \n","    num_inputs = num_outputs = vocab_size\n","\n","    # i_t: input gate layer\n","    W_xi = nn.Parameter(torch.Tensor(num_inputs,num_hiddens))\n","    W_hi = nn.Parameter(torch.Tensor(num_hiddens,num_hiddens))\n","    b_i  = nn.Parameter(torch.Tensor(num_hiddens))\n","\n","    # f_t: forget gate\n","    W_xf = nn.Parameter(torch.Tensor(num_inputs,num_hiddens))\n","    W_hf = nn.Parameter(torch.Tensor(num_hiddens,num_hiddens))\n","    b_f  = nn.Parameter(torch.Tensor(num_hiddens))\n","\n","    # o_t: output gate\n","    W_xo = nn.Parameter(torch.Tensor(num_inputs,num_hiddens))\n","    W_ho = nn.Parameter(torch.Tensor(num_hiddens,num_hiddens))\n","    b_o  = nn.Parameter(torch.Tensor(num_hiddens))\n","\n","    # c_t: Cell state\n","    W_xc = nn.Parameter(torch.Tensor(num_inputs,num_hiddens))\n","    W_hc = nn.Parameter(torch.Tensor(num_hiddens,num_hiddens))\n","    b_c  = nn.Parameter(torch.Tensor(num_hiddens))\n","\n","    # FCN from hidden --> output\n","    W_hq = nn.Parameter(torch.Tensor(num_hiddens,num_outputs))\n","    b_q  = nn.Parameter(torch.Tensor(num_outputs))\n"," \n","    # compute values for initialization\n","    sigma = 1.0 / math.sqrt(num_hiddens)\n","    \n","    # List all the parameters and attach gradients\n","    params = [W_xi, W_hi, b_i, \n","              W_xf, W_hf, b_f, \n","              W_xo, W_ho, b_o, \n","              W_xc, W_hc, b_c,\n","              W_hq, b_q]\n","    for param in params:\n","        # Initialize weights using xavier \n","        param.data.uniform_(-sigma,sigma)\n","        # Requre grad = true\n","        param.requires_grad_(True)\n","\n","    return params"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"5pcMcEav3t24","executionInfo":{"status":"ok","timestamp":1620138139310,"user_tz":240,"elapsed":253,"user":{"displayName":"Justin Brantley","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggf20F3pZnsjHk2N7sW7COs-iVwGzxmFgeR7224=s64","userId":"14927837600437153536"}}},"source":["def lstm(inputs, state, params):\n","    # Get the model params\n","    [W_xi, W_hi, b_i, W_xf, W_hf, b_f, W_xo, W_ho, b_o, W_xc, W_hc, b_c,\n","        W_hq, b_q] = params\n","    # Get the hidden and cell state\n","    (H, C) = state\n","\n","    # Store outputs for each input from a batch \n","    outputs = []\n","\n","    # For each of the inputs in a batch, perform the LSTM operations\n","    for X in inputs:\n","      # Input gate\n","      i_t = torch.sigmoid(X @ W_xi + H @ W_hi + b_i)\n","      # Forget gate\n","      f_t = torch.sigmoid(X @ W_xf + H @ W_hf + b_f)\n","      # Compute c_tilda\n","      c_tilda = torch.tanh(X @ W_xc + H @ W_hc + b_c)\n","      # New cell state C_t = old cell state * f_t + i_t * c_tilda\n","      C_t = (f_t * C) + (i_t * c_tilda)\n","      # Output gate\n","      o_t = torch.sigmoid(X @ W_xo + H @ W_ho + b_o)\n","      # Compute new hidden state\n","      h_t = o_t * torch.tanh(C_t)\n","      \n","      # Update cell and hidden state\n","      H, C = h_t, C_t\n","\n","      # Append output by passing hidden through fully connected output\n","      outputs.append(h_t @ W_hq + b_q)\n","\n","    return torch.cat(outputs, dim=0), (H, C)"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"XeWyBKWaZaLL"},"source":["def get_lstm_peephole_params(vocab_size, num_hiddens):\n","    \n","    num_inputs = num_outputs = vocab_size\n","\n","    # i_t: input gate layer\n","    W_xi = nn.Parameter(torch.Tensor(num_inputs,num_hiddens))\n","    W_hi = nn.Parameter(torch.Tensor(num_hiddens,num_hiddens))\n","    W_ci = nn.Parameter(torch.Tensor(num_hiddens,num_hiddens))\n","    b_i  = nn.Parameter(torch.Tensor(num_hiddens))\n","\n","    # f_t: forget gate\n","    W_xf = nn.Parameter(torch.Tensor(num_inputs,num_hiddens))\n","    W_hf = nn.Parameter(torch.Tensor(num_hiddens,num_hiddens))\n","    W_cf = nn.Parameter(torch.Tensor(num_hiddens,num_hiddens))\n","    b_f  = nn.Parameter(torch.Tensor(num_hiddens))\n","\n","    # o_t: output gate\n","    W_xo = nn.Parameter(torch.Tensor(num_inputs,num_hiddens))\n","    W_ho = nn.Parameter(torch.Tensor(num_hiddens,num_hiddens))\n","    W_co = nn.Parameter(torch.Tensor(num_hiddens,num_hiddens))\n","    b_o  = nn.Parameter(torch.Tensor(num_hiddens))\n","\n","    # c_t: Cell state\n","    W_xc = nn.Parameter(torch.Tensor(num_inputs,num_hiddens))\n","    W_hc = nn.Parameter(torch.Tensor(num_hiddens,num_hiddens))\n","    b_c  = nn.Parameter(torch.Tensor(num_hiddens))\n","\n","    # FCN from hidden --> output\n","    W_hq = nn.Parameter(torch.Tensor(num_hiddens,num_outputs))\n","    b_q  = nn.Parameter(torch.Tensor(num_outputs))\n"," \n","    # compute values for initialization\n","    sigma = 1.0 / math.sqrt(num_hiddens)\n","    \n","    # List all the parameters and attach gradients\n","    params = [W_xi, W_hi, W_ci, b_i, \n","              W_xf, W_hf, W_cf, b_f, \n","              W_xo, W_ho, W_co, b_o, \n","              W_xc, W_hc, b_c,\n","              W_hq, b_q]\n","    for param in params:\n","        # Initialize weights using xavier \n","        param.data.uniform_(-sigma,sigma)\n","        # Requre grad = true\n","        param.requires_grad_(True)\n","\n","    return params\n","\n","def lstm_peephole(inputs, state, params):\n","    # Get the model params\n","    [W_xi, W_hi, W_ci, b_i, W_xf, W_hf, W_cf, b_f, W_xo, W_ho, W_co, b_o, W_xc, W_hc, b_c,\n","        W_hq, b_q] = params\n","    # Get the hidden and cell state\n","    (H, C) = state\n","\n","    # Store outputs for each input from a batch \n","    outputs = []\n","\n","    # For each of the inputs in a batch, perform the LSTM operations\n","    for X in inputs:\n","      # Input gate\n","      i_t = torch.sigmoid(X @ W_xi + C @ W_ci + H @ W_hi + b_i)\n","      # Forget gate\n","      f_t = torch.sigmoid(X @ W_xf + C @ W_cf + H @ W_hf + b_f)\n","      # Compute c_tilda\n","      c_tilda = torch.tanh(X @ W_xc + C @ W_co + H @ W_hc + b_c)\n","      # New cell state C_t = old cell state * f_t + i_t * c_tilda\n","      C_t = (f_t * C) + (i_t * c_tilda)\n","      # Output gate\n","      o_t = torch.sigmoid(X @ W_xo + H @ W_ho + b_o)\n","      # Compute new hidden state\n","      h_t = o_t * torch.tanh(C_t)\n","      \n","      # Update cell and hidden state\n","      H, C = h_t, C_t\n","\n","      # Append output by passing hidden through fully connected output\n","      outputs.append(h_t @ W_hq + b_q)\n","\n","    return torch.cat(outputs, dim=0), (H, C) "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dAebv3u2d4i3","executionInfo":{"status":"ok","timestamp":1620138143398,"user_tz":240,"elapsed":240,"user":{"displayName":"Justin Brantley","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggf20F3pZnsjHk2N7sW7COs-iVwGzxmFgeR7224=s64","userId":"14927837600437153536"}}},"source":["def get_lstm_coupleForgetInput_params(vocab_size, num_hiddens):\n","    \n","    num_inputs = num_outputs = vocab_size\n","\n","    # # i_t: input gate layer\n","    # W_xi = nn.Parameter(torch.Tensor(num_inputs,num_hiddens))\n","    # W_hi = nn.Parameter(torch.Tensor(num_hiddens,num_hiddens))\n","    # b_i  = nn.Parameter(torch.Tensor(num_hiddens))\n","\n","    # f_t: forget gate\n","    W_xf = nn.Parameter(torch.Tensor(num_inputs,num_hiddens))\n","    W_hf = nn.Parameter(torch.Tensor(num_hiddens,num_hiddens))\n","    b_f  = nn.Parameter(torch.Tensor(num_hiddens))\n","\n","    # o_t: output gate\n","    W_xo = nn.Parameter(torch.Tensor(num_inputs,num_hiddens))\n","    W_ho = nn.Parameter(torch.Tensor(num_hiddens,num_hiddens))\n","    b_o  = nn.Parameter(torch.Tensor(num_hiddens))\n","\n","    # c_t: Cell state\n","    W_xc = nn.Parameter(torch.Tensor(num_inputs,num_hiddens))\n","    W_hc = nn.Parameter(torch.Tensor(num_hiddens,num_hiddens))\n","    b_c  = nn.Parameter(torch.Tensor(num_hiddens))\n","\n","    # FCN from hidden --> output\n","    W_hq = nn.Parameter(torch.Tensor(num_hiddens,num_outputs))\n","    b_q  = nn.Parameter(torch.Tensor(num_outputs))\n"," \n","    # compute values for initialization\n","    sigma = 1.0 / math.sqrt(num_hiddens)\n","    \n","    # List all the parameters and attach gradients\n","    params = [W_xf, W_hf, b_f, \n","              W_xo, W_ho, b_o, \n","              W_xc, W_hc, b_c,\n","              W_hq, b_q]\n","    for param in params:\n","        # Initialize weights using xavier \n","        param.data.uniform_(-sigma,sigma)\n","        # Requre grad = true\n","        param.requires_grad_(True)\n","\n","    return params\n","\n","def lstm_coupleForgetInput(inputs, state, params):\n","    # Get the model params\n","    [W_xf, W_hf, b_f, W_xo, W_ho, b_o, W_xc, W_hc, b_c,\n","        W_hq, b_q] = params\n","    # Get the hidden and cell state\n","    (H, C) = state\n","\n","    # Store outputs for each input from a batch \n","    outputs = []\n","\n","    # For each of the inputs in a batch, perform the LSTM operations\n","    for X in inputs:\n","      # Input gate: not used when coupling input/forget\n","      # i_t = torch.sigmoid(X @ W_xi + H @ W_hi + b_i)\n","      # Forget gate\n","      f_t = torch.sigmoid(X @ W_xf + H @ W_hf + b_f)\n","      # Compute c_tilda\n","      c_tilda = torch.tanh(X @ W_xc + H @ W_hc + b_c)\n","      # New cell state C_t = old cell state * f_t + i_t * c_tilda\n","      C_t = (f_t * C) + ((torch.ones_like(f_t) - f_t) * c_tilda)\n","      # Output gate\n","      o_t = torch.sigmoid(X @ W_xo + H @ W_ho + b_o)\n","      # Compute new hidden state\n","      h_t = o_t * torch.tanh(C_t)\n","      \n","      # Update cell and hidden state\n","      H, C = h_t, C_t\n","\n","      # Append output by passing hidden through fully connected output\n","      outputs.append(h_t @ W_hq + b_q)\n","\n","    return torch.cat(outputs, dim=0), (H, C) \n","  "],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"P5fXMTM4i89w","executionInfo":{"status":"ok","timestamp":1620138146068,"user_tz":240,"elapsed":257,"user":{"displayName":"Justin Brantley","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggf20F3pZnsjHk2N7sW7COs-iVwGzxmFgeR7224=s64","userId":"14927837600437153536"}}},"source":["def get_gru_params(vocab_size, num_hiddens):\n","    \n","    num_inputs = num_outputs = vocab_size\n","\n","    # z_t\n","    W_xz = nn.Parameter(torch.Tensor(num_inputs,num_hiddens))\n","    W_hz = nn.Parameter(torch.Tensor(num_hiddens,num_hiddens))\n","\n","    # r_t\n","    W_xr = nn.Parameter(torch.Tensor(num_inputs,num_hiddens))\n","    W_hr = nn.Parameter(torch.Tensor(num_hiddens,num_hiddens))\n","    \n","    # W in h_tilda\n","    W_htildax = nn.Parameter(torch.Tensor(num_inputs,num_hiddens))\n","    W_htildar = nn.Parameter(torch.Tensor(num_hiddens,num_hiddens))\n","    \n","    # FCN from hidden --> output\n","    W_hq = nn.Parameter(torch.Tensor(num_hiddens,num_outputs))\n","    b_q  = nn.Parameter(torch.Tensor(num_outputs))\n"," \n","    # compute values for initialization\n","    sigma = 1.0 / math.sqrt(num_hiddens)\n","    \n","    # List all the parameters and attach gradients\n","    params = [W_xz, W_hz, \n","              W_xr, W_hr, \n","              W_htildax, W_htildar,\n","              W_hq, b_q]\n","    for param in params:\n","        # Initialize weights using xavier \n","        param.data.uniform_(-sigma,sigma)\n","        # Requre grad = true\n","        param.requires_grad_(True)\n","\n","    return params\n","\n","def gru(inputs, state, params):\n","    # Get the model params\n","    [W_xz, W_hz, W_xr, W_hr, W_htildax, W_htildar, W_hq, b_q] = params\n","    \n","    # Get the hidden and cell state - cell state not used for GRU but easier to leave than edit all code\n","    (H, C) = state\n","\n","    # Store outputs for each input from a batch \n","    outputs = []\n","\n","    # For each of the inputs in a batch, perform the LSTM operations\n","    for X in inputs:\n","      # z_t\n","      z_t = torch.sigmoid(X @ W_xz + H @ W_hz)\n","      # r_t\n","      r_t = torch.sigmoid(X @ W_xr + H @ W_hr )\n","      # Compute h_tilda\n","      h_tilda = torch.tanh(X @ W_htildax + (H * r_t) @ W_htildar)\n","      # Compute h_t\n","      h_t = (torch.ones_like(z_t) - z_t) * H + z_t * h_tilda\n","      # Update cell and hidden state\n","      H = h_t\n","\n","      \n","      \n","      # Append output by passing hidden through fully connected output\n","      outputs.append(h_t @ W_hq + b_q)\n","\n","    return torch.cat(outputs, dim=0), (H, C) "],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"8PzTeciC3-e9","executionInfo":{"status":"ok","timestamp":1620138149013,"user_tz":240,"elapsed":346,"user":{"displayName":"Justin Brantley","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggf20F3pZnsjHk2N7sW7COs-iVwGzxmFgeR7224=s64","userId":"14927837600437153536"}}},"source":["# Build the LSTM model from scratch\n","class LSTMScratch:  \n","    \n","    \"\"\"An LSTM Model implemented from scratch.\"\"\"\n","    \n","    def __init__(self, vocab_size, num_hiddens, get_params, forward_fn):\n","\n","        self.vocab_size, self.num_hiddens, self.forward_fn = vocab_size, num_hiddens, forward_fn\n","        # Function to initialize all the parameters in the LSTM (defined by you!)\n","        self.params = get_params(vocab_size, num_hiddens)\n","\n","    def __call__(self, X, state):\n","        # Calls the forward function \n","        \n","        # Convert X to a one-hot vector for each character instead of an Embedding\n","        X = F.one_hot(X.T, self.vocab_size).type(torch.float32)\n","        \n","        # Function to pass inputs to the model (defined by you!)\n","        return self.forward_fn(X, state, self.params)\n","\n","    def begin_state(self, batch_size):\n","        # Initialize the hidden state and cell state with zeros\n","        return (torch.zeros((batch_size, num_hiddens), device=device),\n","            torch.zeros((batch_size, num_hiddens), device=device))"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"PYx7nc3u4XJz","executionInfo":{"status":"ok","timestamp":1620138151465,"user_tz":240,"elapsed":407,"user":{"displayName":"Justin Brantley","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggf20F3pZnsjHk2N7sW7COs-iVwGzxmFgeR7224=s64","userId":"14927837600437153536"}}},"source":["#@title Training Functions (Run Me!)\n","# Clip the gradients\n","def grad_clipping(net, theta):\n","    \"\"\"Clip the gradient.\"\"\"\n","    params = net.params\n","    norm = torch.sqrt(sum(torch.sum((p.grad**2)) for p in params))\n","    if norm > theta:\n","        for param in params:\n","            param.grad[:] *= theta / norm\n","\n","# Optimization Minibatch SGD\n","def sgd(params, lr, batch_size):\n","    \"\"\"Minibatch stochastic gradient descent.\"\"\"\n","    with torch.no_grad():\n","        for param in params:\n","            param -= lr * param.grad / batch_size\n","            param.grad.zero_()\n","\n","# Training in each epoch\n","def train_epoch(net, train_iter, loss, optimizer):\n","    \"\"\"Train a net within one epoch (defined in Chapter 8).\"\"\"\n","\n","    state, timer = None, d2l.Timer()\n","    metric = d2l.Accumulator(2)  # Sum of training loss, no. of tokens\n","    \n","    # For every barch in dataloader\n","    for X, Y in train_iter:\n","        if state is None:\n","            # Initialize `state` when either it is the first iteration or\n","            # using random sampling\n","            state = net.begin_state(batch_size=X.shape[0])\n","        else:\n","            # `state` is a tuple of tensors for our custom scratch implementation\n","            for s in state:\n","                s.detach_()\n","\n","        # Pass input and state to the model\n","        y = Y.T.reshape(-1)\n","        X, y = X.to(device), y.to(device)\n","        y_hat, state = net(X, state)\n","\n","        # Compute loss, backpropagate and clip the gradient\n","        l = loss(y_hat, y.long()).mean()\n","        l.backward()\n","        grad_clipping(net, 1)\n","\n","        # Step the optimizer\n","        optimizer(batch_size=1)\n","\n","        # Loss metric append    \n","        metric.add(l * y.numel(), y.numel())\n","    return math.exp(metric[0] / metric[1]), metric[1] / timer.stop()\n","\n","# Predict function\n","def predict(prefix, num_preds, net, vocab):  \n","    \"\"\"Generate new characters following the `prefix`.\"\"\"    \n","    # Initialize and tensorize input\n","    state = net.begin_state(batch_size=1)\n","    outputs = [vocab[prefix[0]]]\n","    get_input = lambda: torch.tensor([outputs[-1]], device=device).reshape(\n","        (1, 1))\n","    \n","    for y in prefix[1:]:  # Warm-up period\n","        _, state = net(get_input(), state)\n","        outputs.append(vocab[y])\n","    \n","    for _ in range(num_preds):  # Predict `num_preds` steps\n","        y, state = net(get_input(), state)\n","        outputs.append(int(y.argmax(dim=1).reshape(1)))\n","    \n","    return ''.join([vocab.idx_to_token[i] for i in outputs])\n","\n","# Train function\n","def train(net, train_iter, vocab, lr, num_epochs):\n","    \"\"\"Train a model.\"\"\"\n","\n","    # Loss function and loss visualization tool \n","    loss = nn.CrossEntropyLoss()\n","    animator = d2l.Animator(xlabel='epoch', ylabel='perplexity', legend=['train'], xlim=[10, num_epochs])\n","    \n","    # Optimizer\n","    optimizer = lambda batch_size: sgd(net.params, lr, batch_size)\n","\n","    # Train and predict over each epoch\n","    for epoch in range(num_epochs):\n","        ppl, speed = train_epoch(net, train_iter, loss, optimizer)\n","        \n","        # Plot every 10 epochs\n","        if (epoch + 1) % 10 == 0:\n","            print(predict('time traveller', 50, net, vocab))\n","            animator.add(epoch + 1, [ppl])\n","    \n","    # Final predictions\n","    print(f'perplexity {ppl:.1f}, {speed:.1f} tokens/sec on {str(device)}')\n","    print(predict('time traveller', 50, net, vocab))\n","    print(predict('traveller', 50, net, vocab))"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"MegqMA8pGTXe","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1620138157763,"user_tz":240,"elapsed":1092,"user":{"displayName":"Justin Brantley","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggf20F3pZnsjHk2N7sW7COs-iVwGzxmFgeR7224=s64","userId":"14927837600437153536"}},"outputId":"6f174b99-9f5d-4894-b712-d865b0bce505"},"source":["batch_size, num_steps = 32, 50\n","\n","train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)"],"execution_count":9,"outputs":[{"output_type":"stream","text":["Downloading ../data/timemachine.txt from http://d2l-data.s3-accelerate.amazonaws.com/timemachine.txt...\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"cKDBy3urwq2r"},"source":["train_iter.__dict__"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LcOy5DAR3wLD","executionInfo":{"status":"ok","timestamp":1620138389717,"user_tz":240,"elapsed":455,"user":{"displayName":"Justin Brantley","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggf20F3pZnsjHk2N7sW7COs-iVwGzxmFgeR7224=s64","userId":"14927837600437153536"}}},"source":["vocab_size, num_hiddens, device = len(vocab), 256, torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","num_epochs, lr = 500, 1\n","\n","model = LSTMScratch(len(vocab), num_hiddens, get_gru_params, gru)\n","train(model, train_iter, vocab, lr, num_epochs)"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gX8QwKz0xf6T","executionInfo":{"status":"ok","timestamp":1620138406689,"user_tz":240,"elapsed":922,"user":{"displayName":"Justin Brantley","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggf20F3pZnsjHk2N7sW7COs-iVwGzxmFgeR7224=s64","userId":"14927837600437153536"}},"outputId":"f96804c2-502c-40cd-809e-ec7899cf1078"},"source":["print(model.params)"],"execution_count":14,"outputs":[{"output_type":"stream","text":["[Parameter containing:\n","tensor([[-0.0565, -0.0284,  0.0555,  ..., -0.0371, -0.0210, -0.0386],\n","        [ 0.0262,  0.0036,  0.0177,  ...,  0.0095,  0.0262,  0.0458],\n","        [-0.0075,  0.0222,  0.0095,  ..., -0.0252,  0.0078, -0.0473],\n","        ...,\n","        [ 0.0484, -0.0228,  0.0109,  ...,  0.0413,  0.0353,  0.0141],\n","        [-0.0573, -0.0264,  0.0384,  ..., -0.0171,  0.0509,  0.0092],\n","        [ 0.0014,  0.0378,  0.0547,  ...,  0.0561, -0.0164,  0.0293]],\n","       requires_grad=True), Parameter containing:\n","tensor([[-0.0223,  0.0408,  0.0353,  ..., -0.0111,  0.0082, -0.0200],\n","        [-0.0594,  0.0564, -0.0236,  ...,  0.0222, -0.0415,  0.0352],\n","        [-0.0141, -0.0103, -0.0269,  ..., -0.0026,  0.0456, -0.0060],\n","        ...,\n","        [-0.0317, -0.0594, -0.0159,  ...,  0.0096, -0.0624,  0.0097],\n","        [ 0.0369,  0.0284,  0.0323,  ..., -0.0006, -0.0616, -0.0105],\n","        [-0.0213,  0.0236,  0.0355,  ...,  0.0407,  0.0251,  0.0141]],\n","       requires_grad=True), Parameter containing:\n","tensor([[ 0.0598, -0.0334, -0.0456,  ...,  0.0560,  0.0056, -0.0255],\n","        [-0.0579, -0.0065,  0.0094,  ...,  0.0051,  0.0419,  0.0414],\n","        [-0.0356, -0.0009, -0.0351,  ...,  0.0317,  0.0244,  0.0144],\n","        ...,\n","        [-0.0500,  0.0331, -0.0334,  ...,  0.0438, -0.0213,  0.0405],\n","        [ 0.0130, -0.0437,  0.0129,  ..., -0.0417, -0.0156, -0.0233],\n","        [-0.0414, -0.0338, -0.0131,  ...,  0.0025,  0.0574, -0.0176]],\n","       requires_grad=True), Parameter containing:\n","tensor([[ 0.0347, -0.0462,  0.0357,  ...,  0.0391,  0.0012,  0.0477],\n","        [-0.0549, -0.0384,  0.0235,  ...,  0.0553, -0.0243,  0.0534],\n","        [ 0.0167, -0.0516, -0.0225,  ..., -0.0605,  0.0428, -0.0357],\n","        ...,\n","        [-0.0360,  0.0621,  0.0037,  ...,  0.0246,  0.0115, -0.0284],\n","        [ 0.0562,  0.0444,  0.0203,  ...,  0.0123,  0.0283,  0.0029],\n","        [ 0.0078, -0.0297, -0.0193,  ...,  0.0158,  0.0069,  0.0448]],\n","       requires_grad=True), Parameter containing:\n","tensor([[ 0.0235,  0.0144, -0.0400,  ...,  0.0170, -0.0327, -0.0442],\n","        [ 0.0139, -0.0073,  0.0145,  ...,  0.0067,  0.0328,  0.0461],\n","        [ 0.0376,  0.0294, -0.0266,  ...,  0.0392, -0.0061, -0.0007],\n","        ...,\n","        [ 0.0351, -0.0504, -0.0453,  ..., -0.0087,  0.0055,  0.0233],\n","        [ 0.0197, -0.0298,  0.0549,  ..., -0.0624, -0.0441, -0.0555],\n","        [ 0.0186,  0.0007,  0.0136,  ...,  0.0199,  0.0488, -0.0374]],\n","       requires_grad=True), Parameter containing:\n","tensor([[ 0.0025,  0.0462,  0.0519,  ...,  0.0503,  0.0240,  0.0415],\n","        [ 0.0412,  0.0303, -0.0413,  ...,  0.0383, -0.0227,  0.0266],\n","        [-0.0234, -0.0526, -0.0429,  ...,  0.0450, -0.0238,  0.0040],\n","        ...,\n","        [-0.0494, -0.0293, -0.0431,  ..., -0.0615, -0.0038, -0.0051],\n","        [-0.0505,  0.0183, -0.0226,  ..., -0.0372,  0.0557,  0.0292],\n","        [ 0.0319,  0.0136,  0.0397,  ..., -0.0588,  0.0544, -0.0152]],\n","       requires_grad=True), Parameter containing:\n","tensor([[ 0.0019,  0.0347,  0.0622,  ..., -0.0492, -0.0163, -0.0490],\n","        [-0.0125, -0.0259,  0.0054,  ...,  0.0171,  0.0030,  0.0378],\n","        [ 0.0232, -0.0179,  0.0380,  ..., -0.0179, -0.0332,  0.0083],\n","        ...,\n","        [-0.0551, -0.0118,  0.0026,  ...,  0.0592, -0.0503,  0.0438],\n","        [-0.0054,  0.0075,  0.0377,  ...,  0.0404, -0.0470, -0.0227],\n","        [-0.0112,  0.0079,  0.0108,  ..., -0.0573, -0.0535, -0.0571]],\n","       requires_grad=True), Parameter containing:\n","tensor([-0.0449,  0.0252,  0.0254,  0.0387,  0.0174, -0.0388, -0.0508,  0.0359,\n","         0.0187, -0.0453,  0.0164,  0.0434,  0.0346, -0.0086, -0.0299, -0.0185,\n","         0.0608,  0.0432, -0.0296, -0.0411, -0.0086, -0.0340, -0.0119, -0.0573,\n","         0.0033, -0.0248, -0.0243,  0.0262], requires_grad=True)]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"fykfRPEwHiai","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617132074676,"user_tz":240,"elapsed":242260,"user":{"displayName":"Justin Brantley","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggf20F3pZnsjHk2N7sW7COs-iVwGzxmFgeR7224=s64","userId":"14927837600437153536"}},"outputId":"02246324-7188-47dc-ed69-63666280ce4c"},"source":["# Add your initial primer strings to this list for testing\n","sentences = ['time traveller',\n","             'traveller',\n","             'machine'\n","\n","]\n","\n","for sentence in sentences:\n","    print(predict(sentence, 50, model, vocab))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["time traveller ffree if a mo in all dlancl this is all phased un\n","travelleryou can show black is white by argument said filby\n","machine i wo ch anguttee ote himentron puotion anot the t\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"APi-IM5EVh7K"},"source":["Summarize the results of your ablation study. What changes did you make? How was the performance affacted? Provide reasoning for your observations. (200-400 words)"]},{"cell_type":"code","metadata":{"id":"5CmtmN-j5Pk9"},"source":["section3_ans = \"\"\" 1. Xavier initialization of weights, standard LSTM,                             500 epoch, LR = 1   : Perplexity goes from ~17 --> 1.1. Output of test has words that make sense but many are nonsense, particularly at end of the sentence.\n","                   2. Xavier initialization of weights, standard LSTM,                             500 epoch, LR = 0.1 : Perplexity goes from ~25 --> 11.2. Output of test has mostly the word 'the'\n","                   3. Xavier initialization of weights, LSTM w/peepholes,                          500 epoch, LR = 1   : Perplexity goes from ~17 --> 1.0. Peepholes on all gates. Converged by ~250 epochs. Output of test has words that make sense but end of sentence gets sloppy. Lost spaces leading to combined words. Otherwise better than w/o peepholes\n","                   4. Xavier initialization of weights, LSTM w/coupled input and forget,           500 epoch, LR = 1   : Perplexity goes from ~18 --> 1.1. Output of test has words that make sense but end of sentence is not great. Also lost spaces but not too bad. \n","                   5. Xavier initialization of weights, Changed LSTM to GRU,                       500 epoch, LR = 1   : Perplexity goes from ~16 --> 1.1. Output of test has words that make sense but in some cases things went wrong quickly. When a mistake was made early the rest of the sentence was no longer correct words.\n","                   6. Xavier initialization of weights, Changed LSTM to GRU w/dropout(0.5) b4 FCN, 500 epoch, LR = 1   : Perplexity goes from ~18 --> 1.8. Output of test has only a word or two at the beginning that make sense but otherwise are not very coherent. \n","                   7. Xavier initialization of weights, Changed LSTM to GRU w/dropout(X,0.5),      500 epoch, LR = 1   : Perplexity goes from ~18 --> 8.1. Output of test is bad and gets stuck repeating words starting with 't', such as 'time','the','then','thoe','toe',etc \"\"\" #@param {type:\"string\"}"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"le-1JMKWRVxy"},"source":["## Section 4: Know-Your-Pod Better"]},{"cell_type":"markdown","metadata":{"id":"_u3z3BIcRZkG"},"source":["Discuss with two other members of your pod. What is their favorite movie scene (not just the whole movie, but describe the scene!) of all time? Describe the scene and why they like it so much. (~100 words each)"]},{"cell_type":"code","metadata":{"cellView":"form","id":"AbIh2_ca44DK"},"source":["section4_ans = 'Karan: end of inception where everything comes together. Abhisheck: Scene in children of men that is a neat action scene' #@param {type:\"string\"}"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gbgTqgB5rH_W"},"source":["---\n","# Submission\n","Once you're done, click on 'Share' and add the link to the box below.\n"]},{"cell_type":"code","metadata":{"id":"oeC-YYf85J2P","colab":{"base_uri":"https://localhost:8080/","height":421},"executionInfo":{"status":"ok","timestamp":1617132312270,"user_tz":240,"elapsed":339,"user":{"displayName":"Justin Brantley","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggf20F3pZnsjHk2N7sW7COs-iVwGzxmFgeR7224=s64","userId":"14927837600437153536"}},"outputId":"85c825b8-c251-4fbd-ff73-a768fc4df221"},"source":["import time\n","import numpy as np\n","import urllib.parse\n","from IPython.display import IFrame\n","\n","t7 = time.time()\n","\n","#@markdown #Run Cell to Show Airtable Form\n","#@markdown ##**Confirm your answers and then click \"Submit\"**\n","\n","\n","def prefill_form(src, fields: dict):\n","  '''\n","  src: the original src url to embed the form\n","  fields: a dictionary of field:value pairs,\n","  e.g. {\"pennkey\": my_pennkey, \"location\": my_location}\n","  '''\n","  prefill_fields = {}\n","  for key in fields:\n","      new_key = 'prefill_' + key\n","      prefill_fields[new_key] = fields[key]\n","  prefills = urllib.parse.urlencode(prefill_fields)\n","  src = src + prefills\n","  return src\n","\n","\n","\n","#autofill fields if they are not present\n","#a missing pennkey and pod will result in an Airtable warning\n","#which is easily fixed user-side.\n","try: my_pennkey;\n","except NameError: my_pennkey = \"\"\n","\n","try: my_pod;\n","except NameError: my_pod = \"Select\"\n","\n","try: section1_ans;\n","except NameError: section1_ans = \"\"\n","\n","try: section2_ans;\n","except NameError: section2_ans = \"\"\n","\n","try: section3_ans;\n","except NameError: section3_ans = \"\"\n","\n","try: section4_ans;\n","except NameError: section4_ans = \"\"\n","\n","\n","fields = {\"pennkey\": my_pennkey,\n","          \"pod\": my_pod,\n","          \"section1_ans\": section1_ans,\n","          \"section2_ans\": section2_ans,\n","          \"section3_ans\": section3_ans,\n","          \"section4_ans\": section4_ans}\n","\n","src = \"https://airtable.com/shrZdjQjhvUSHVgW5?\"\n","\n","\n","#now instead of the original source url, we do: src = prefill_form(src, fields)\n","display(IFrame(src = prefill_form(src, fields), width = 800, height = 400))"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","        <iframe\n","            width=\"800\"\n","            height=\"400\"\n","            src=\"https://airtable.com/shrZdjQjhvUSHVgW5?prefill_pennkey=jabrantl&prefill_pod=superfluous-lyrebird&prefill_section1_ans=This+problem+seems+like+an+almost+insurmountable+challenge.+To+me%2C+~90%25+accuracy+detection+of+hate+speech+is+quite+impressive+considering+all+of+the+nuance+and+slang+that+is+often+used+in+hateful+speech.+Examples+of+direct+threats+or+hate+speech+seems+somewhat+simple+to+detect%2C+but+in+many+cases%2C+hate+speech%2Fwriting+can+occur+through+difficult+to+recognize+bullying.+For+example%2C+the+idea+of+combining+ambiguous+text+onto+images+to+create+hateful+memes+seems+like+an+incredibly+difficult+challenge.+The+one+concern+in+all+of+this+is+how+the+speech+is+flagged+as+%27hateful%27+in+the+first+place.+In+fact%2C+many+people+%28particularly+those+in+more+conservative+circles%29+often+complain+about+their+speech+being+flagged+as+hateful+and+companies+overstepping+and+violating+the+1st+amendment.+However%2C++hate+speech+is+not+protected+by+the+1st+amendment++%2C+so+it+is+right+to+be+removed.+On+the+other+hand%2C+the+definition+of+hate+speech+can+be+manipulated+to+reflect+the+bias+of+those+doing+the+labeling.+I+like+the+idea+of+self-supervised+learning+as+a+way+to+identify+potential+hate+speech+w%2Fo+humans+in+the+loop%2C+but+this+could+also+lead+to+accidental+suppression+of+speech+that+was+not+actually+hate+speech+or+was+quoting+or+rebuttal+to+hate+speech.+++The+second+part+of+the+article+is+more+challenging+for+me.+I+see+how+the+discussion+about+language+styles+being+perceived+as+%27wrong%27+in+a+social+media+environment+is+bad%2C+but+I+still+struggle+with+the+idea+of+having+vernacular+that+uses+complete+improper+grammar+as+being+correct+English.+I+also+sometimes+struggle+with+the+criticism+that+you+have+to+%22talk+White%22+to+be+accepted+as+talking+correctly.+My+family+speaks+English+and+Spanish+and+in+both+languages%2C+grammar+and+structure+are+still+present+and+somewhat+interchangeable+between+both.+I+find+the+argument+about+double+negatives+to+be+weak+unless+it+purely+hinges+on+the+word+%27logic%27.+Maybe+illogical+is+incorrect%2C+but+just+because+something+is+ok+in+another+language+does+not+mean+that+it+has+to+hold+for+a+different+one.+Just+because+you+can+do+it+in+Italian+does+not+mean+that+it+is+also+ok+in+English.+That+would+also+mean+that+in++&prefill_section2_ans=Clearly+the+method+for+labeling+text+as+toxic+is+completely+flawed+and+has+huge+racial%2C+gender%2C+and+sexual+identity+bias.+Although+I+do+not+know+the+technical+side+of+how+these+expressions+are+being+labeled+as+toxic%2C+it+is+not+clear+to+me+at+all+how+this+could+occur+in+the+first+place+w%2Fo+having+seriously+corrupted+training+data--i.e.%2C+people+were+labeling+things+with+their+own+biases+being+very+strong.+I+find+the+second+part+of+the+article+to+be+less+compelling%2C+but+perhaps+that+discussion+is+for+a+later+time.+++&prefill_section3_ans=+1.+Xavier+initialization+of+weights%2C+standard+LSTM%2C+++++++++++++++++++++++++++++500+epoch%2C+LR+%3D+1+++%3A+Perplexity+goes+from+~17+--%3E+1.1.+Output+of+test+has+words+that+make+sense+but+many+are+nonsense%2C+particularly+at+end+of+the+sentence.%0A+++++++++++++++++++2.+Xavier+initialization+of+weights%2C+standard+LSTM%2C+++++++++++++++++++++++++++++500+epoch%2C+LR+%3D+0.1+%3A+Perplexity+goes+from+~25+--%3E+11.2.+Output+of+test+has+mostly+the+word+%27the%27%0A+++++++++++++++++++3.+Xavier+initialization+of+weights%2C+LSTM+w%2Fpeepholes%2C++++++++++++++++++++++++++500+epoch%2C+LR+%3D+1+++%3A+Perplexity+goes+from+~17+--%3E+1.0.+Peepholes+on+all+gates.+Converged+by+~250+epochs.+Output+of+test+has+words+that+make+sense+but+end+of+sentence+gets+sloppy.+Lost+spaces+leading+to+combined+words.+Otherwise+better+than+w%2Fo+peepholes%0A+++++++++++++++++++4.+Xavier+initialization+of+weights%2C+LSTM+w%2Fcoupled+input+and+forget%2C+++++++++++500+epoch%2C+LR+%3D+1+++%3A+Perplexity+goes+from+~18+--%3E+1.1.+Output+of+test+has+words+that+make+sense+but+end+of+sentence+is+not+great.+Also+lost+spaces+but+not+too+bad.+%0A+++++++++++++++++++5.+Xavier+initialization+of+weights%2C+Changed+LSTM+to+GRU%2C+++++++++++++++++++++++500+epoch%2C+LR+%3D+1+++%3A+Perplexity+goes+from+~16+--%3E+1.1.+Output+of+test+has+words+that+make+sense+but+in+some+cases+things+went+wrong+quickly.+When+a+mistake+was+made+early+the+rest+of+the+sentence+was+no+longer+correct+words.%0A+++++++++++++++++++6.+Xavier+initialization+of+weights%2C+Changed+LSTM+to+GRU+w%2Fdropout%280.5%29+b4+FCN%2C+500+epoch%2C+LR+%3D+1+++%3A+Perplexity+goes+from+~18+--%3E+1.8.+Output+of+test+has+only+a+word+or+two+at+the+beginning+that+make+sense+but+otherwise+are+not+very+coherent.+%0A+++++++++++++++++++7.+Xavier+initialization+of+weights%2C+Changed+LSTM+to+GRU+w%2Fdropout%28X%2C0.5%29%2C++++++500+epoch%2C+LR+%3D+1+++%3A+Perplexity+goes+from+~18+--%3E+8.1.+Output+of+test+is+bad+and+gets+stuck+repeating+words+starting+with+%27t%27%2C+such+as+%27time%27%2C%27the%27%2C%27then%27%2C%27thoe%27%2C%27toe%27%2Cetc%0A++++++++++++++++&prefill_section4_ans=Karan%3A+end+of+inception+where+everything+comes+together.+Abhisheck%3A+Scene+in+children+of+men+that+is+a+neat+action+scene\"\n","            frameborder=\"0\"\n","            allowfullscreen\n","        ></iframe>\n","        "],"text/plain":["<IPython.lib.display.IFrame at 0x7f20184fd5d0>"]},"metadata":{"tags":[]}}]}]}